{"timestamp":"2025-09-03T15:10:02.204908","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-03T15:10:02.207737","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/spark_submit_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-03T15:10:02.233648","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:02.234388","level":"info","event":"Running command: ['/usr/bin/bash', '-c', '\\n        spark-submit         --master spark://spark-master:7077         --conf spark.hadoop.fs.s3a.access.key=telcoaz         --conf spark.hadoop.fs.s3a.secret.key=Telco12345         --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000         --conf spark.hadoop.fs.s3a.path.style.access=true \\t--conf spark.eventlog.enabled=false         /opt/airflow/dags/hello_spark.py\\n        ']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:02.264091","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:08.910862","level":"info","event":"25/09/03 15:10:08 INFO SparkContext: Running Spark version 3.5.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:08.913572","level":"info","event":"25/09/03 15:10:08 INFO SparkContext: OS info Linux, 6.14.0-1012-aws, amd64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:08.915201","level":"info","event":"25/09/03 15:10:08 INFO SparkContext: Java version 17.0.16","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.032974","level":"info","event":"25/09/03 15:10:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.205523","level":"info","event":"25/09/03 15:10:09 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.206471","level":"info","event":"25/09/03 15:10:09 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.208014","level":"info","event":"25/09/03 15:10:09 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.211374","level":"info","event":"25/09/03 15:10:09 INFO SparkContext: Submitted application: ProcessingTask","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.217892","level":"info","event":"25/09/03 15:10:09 INFO SparkContext: Spark configuration:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.218144","level":"info","event":"spark.app.name=ProcessingTask","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.218261","level":"info","event":"spark.app.startTime=1756912208892","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.218366","level":"info","event":"spark.app.submitTime=1756912207624","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.218464","level":"info","event":"spark.delta.logStore.class=org.apache.spark.sql.delta.storage.S3SingleDriverLogStore","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.218575","level":"info","event":"spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.218706","level":"info","event":"spark.eventLog.dir=/opt/spark/history","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.218834","level":"info","event":"spark.eventLog.enabled=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.218933","level":"info","event":"spark.eventlog.enabled=false","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.219038","level":"info","event":"spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.219136","level":"info","event":"spark.hadoop.fs.s3a.access.key=*********(redacted)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.219229","level":"info","event":"spark.hadoop.fs.s3a.endpoint=http://minio:9000","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.219324","level":"info","event":"spark.hadoop.fs.s3a.fast.upload=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.219417","level":"info","event":"spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.219521","level":"info","event":"spark.hadoop.fs.s3a.multiobjectdelete.enable=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.219618","level":"info","event":"spark.hadoop.fs.s3a.path.style.access=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.223746","level":"info","event":"spark.hadoop.fs.s3a.secret.key=*********(redacted)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.224056","level":"info","event":"spark.history.fs.logDirectory=s3a://spark/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.224211","level":"info","event":"spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.224313","level":"info","event":"spark.jars.repositories=https://packages.confluent.io/maven/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.224407","level":"info","event":"spark.logConf=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.224499","level":"info","event":"spark.master=spark://spark-master:7077","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.224596","level":"info","event":"spark.network.timeout=10000s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.224685","level":"info","event":"spark.rdd.compress=True","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.224793","level":"info","event":"spark.serializer.objectStreamReset=100","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.224874","level":"info","event":"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.224962","level":"info","event":"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.225045","level":"info","event":"spark.sql.files.ignoreMissingFiles=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.225141","level":"info","event":"spark.sql.sources.partitionOverwriteMode=dynamic","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.225244","level":"info","event":"spark.submit.deployMode=client","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.225341","level":"info","event":"spark.submit.pyFiles=","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.257453","level":"info","event":"25/09/03 15:10:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.268987","level":"info","event":"25/09/03 15:10:09 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.269252","level":"info","event":"25/09/03 15:10:09 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.346111","level":"info","event":"25/09/03 15:10:09 INFO SecurityManager: Changing view acls to: airflow","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.346365","level":"info","event":"25/09/03 15:10:09 INFO SecurityManager: Changing modify acls to: airflow","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.348168","level":"info","event":"25/09/03 15:10:09 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.348335","level":"info","event":"25/09/03 15:10:09 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.348458","level":"info","event":"25/09/03 15:10:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.729201","level":"info","event":"25/09/03 15:10:09 INFO Utils: Successfully started service 'sparkDriver' on port 44733.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.767005","level":"info","event":"25/09/03 15:10:09 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.830556","level":"info","event":"25/09/03 15:10:09 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.858483","level":"info","event":"25/09/03 15:10:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.860930","level":"info","event":"25/09/03 15:10:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.867398","level":"info","event":"25/09/03 15:10:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.900460","level":"info","event":"25/09/03 15:10:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d86f01d0-7ece-4e04-8063-021933851063","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.923968","level":"info","event":"25/09/03 15:10:09 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:09.947124","level":"info","event":"25/09/03 15:10:09 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.209960","level":"info","event":"25/09/03 15:10:10 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.301154","level":"info","event":"25/09/03 15:10:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.583381","level":"info","event":"25/09/03 15:10:10 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.673446","level":"info","event":"25/09/03 15:10:10 INFO TransportClientFactory: Successfully created connection to spark-master/172.22.0.3:7077 after 44 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.839136","level":"info","event":"25/09/03 15:10:10 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250903151010-0013","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.845518","level":"info","event":"25/09/03 15:10:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250903151010-0013/0 on worker-20250903081951-172.22.0.5-43269 (172.22.0.5:43269) with 2 core(s)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.853406","level":"info","event":"25/09/03 15:10:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20250903151010-0013/0 on hostPort 172.22.0.5:43269 with 2 core(s), 1024.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.862478","level":"info","event":"25/09/03 15:10:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35991.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.864013","level":"info","event":"25/09/03 15:10:10 INFO NettyBlockTransferService: Server created on 6dcc95cec7e0:35991","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.867375","level":"info","event":"25/09/03 15:10:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.877778","level":"info","event":"25/09/03 15:10:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 6dcc95cec7e0, 35991, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.889965","level":"info","event":"25/09/03 15:10:10 INFO BlockManagerMasterEndpoint: Registering block manager 6dcc95cec7e0:35991 with 434.4 MiB RAM, BlockManagerId(driver, 6dcc95cec7e0, 35991, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.900647","level":"info","event":"25/09/03 15:10:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 6dcc95cec7e0, 35991, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.908002","level":"info","event":"25/09/03 15:10:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 6dcc95cec7e0, 35991, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:10.960959","level":"info","event":"25/09/03 15:10:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250903151010-0013/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:11.745733","level":"info","event":"25/09/03 15:10:11 INFO SingleEventLogFileWriter: Logging events to file:/opt/spark/history/app-20250903151010-0013.inprogress","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:12.491846","level":"info","event":"25/09/03 15:10:12 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:13.256394","level":"info","event":"25/09/03 15:10:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:13.264799","level":"info","event":"25/09/03 15:10:13 INFO SharedState: Warehouse path is 'file:/tmp/airflowtmpul3ui2u6/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:15.868185","level":"info","event":"25/09/03 15:10:15 INFO InMemoryFileIndex: It took 195 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:17.227580","level":"info","event":"25/09/03 15:10:17 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:17.316858","level":"info","event":"25/09/03 15:10:17 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:17.318548","level":"info","event":"25/09/03 15:10:17 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:17.319084","level":"info","event":"25/09/03 15:10:17 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:17.320929","level":"info","event":"25/09/03 15:10:17 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:17.325937","level":"info","event":"25/09/03 15:10:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:17.510253","level":"info","event":"25/09/03 15:10:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 105.5 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:17.574926","level":"info","event":"25/09/03 15:10:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:17.579888","level":"info","event":"25/09/03 15:10:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 6dcc95cec7e0:35991 (size: 38.4 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:17.590931","level":"info","event":"25/09/03 15:10:17 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:17.665966","level":"info","event":"25/09/03 15:10:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:17.669930","level":"info","event":"25/09/03 15:10:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:19.003956","level":"info","event":"25/09/03 15:10:19 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.22.0.5:51726) with ID 0,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:19.280547","level":"info","event":"25/09/03 15:10:19 INFO BlockManagerMasterEndpoint: Registering block manager 172.22.0.5:33063 with 434.4 MiB RAM, BlockManagerId(0, 172.22.0.5, 33063, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:19.439602","level":"info","event":"25/09/03 15:10:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 7768 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:20.080098","level":"info","event":"25/09/03 15:10:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.22.0.5:33063 (size: 38.4 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.375965","level":"info","event":"25/09/03 15:10:21 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.22.0.5 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.376212","level":"info","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.376325","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.376425","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:383)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.376524","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.376617","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.376726","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.376827","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.376907","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.376978","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.377046","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.377122","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.377201","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.377292","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.377375","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.377455","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.377534","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.377603","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.377675","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.384895","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.385079","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.385181","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.385278","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.385371","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.385466","level":"info","event":"Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/yellow_tripdata_2024-01.parquet does not exist","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.385560","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.385653","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.390324","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.390555","level":"info","event":"\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.390688","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.397801","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.397944","level":"info","event":"\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.398043","level":"info","event":"\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.398138","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.398227","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.398314","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.398409","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.398496","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.398587","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.398684","level":"info","event":"\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.399969","level":"info","event":"\tat scala.util.Success.$anonfun$map$1(Try.scala:255)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.400092","level":"info","event":"\tat scala.util.Success.map(Try.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.400202","level":"info","event":"\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.400293","level":"info","event":"\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.400377","level":"info","event":"\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.400455","level":"info","event":"\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.400534","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.400626","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.403805","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.404036","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.404165","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.404255","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.404344","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.404424","level":"info","event":"25/09/03 15:10:21 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 7768 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.629965","level":"info","event":"25/09/03 15:10:21 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on 172.22.0.5, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.638481","level":"info","event":"25/09/03 15:10:21 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 7768 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.771924","level":"info","event":"25/09/03 15:10:21 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on 172.22.0.5, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.779261","level":"info","event":"25/09/03 15:10:21 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 7768 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.875467","level":"info","event":"25/09/03 15:10:21 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on 172.22.0.5, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.877410","level":"info","event":"25/09/03 15:10:21 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.881515","level":"info","event":"25/09/03 15:10:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.888565","level":"info","event":"25/09/03 15:10:21 INFO TaskSchedulerImpl: Cancelling stage 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.893057","level":"info","event":"25/09/03 15:10:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.22.0.5 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.893315","level":"info","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.893430","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.893534","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:383)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.893617","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.893701","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.893816","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.893896","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.893978","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.894060","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.894141","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.894218","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.894295","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.894374","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.894452","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.894531","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.894613","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.894692","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.897929","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.898090","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.898204","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.898308","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.898403","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.898480","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.898557","level":"info","event":"Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/yellow_tripdata_2024-01.parquet does not exist","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.898649","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.898775","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.898864","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.898941","level":"info","event":"\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.899013","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.899085","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.899161","level":"info","event":"\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.899234","level":"info","event":"\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.899307","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.899382","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.899467","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.899541","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.899614","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.899693","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.909328","level":"info","event":"\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.909464","level":"info","event":"\tat scala.util.Success.$anonfun$map$1(Try.scala:255)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.909551","level":"info","event":"\tat scala.util.Success.map(Try.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.909634","level":"info","event":"\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.909734","level":"info","event":"\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.909828","level":"info","event":"\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.909920","level":"info","event":"\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.909998","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.910077","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.910154","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.910233","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.910309","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.910386","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.910463","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.910543","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.910624","level":"info","event":"25/09/03 15:10:21 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) failed in 4.543 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.22.0.5 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.915802","level":"info","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.916145","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.916268","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:383)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.916367","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.916464","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.916563","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.916657","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.920534","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.920791","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.920906","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.921000","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.921092","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.921179","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.921268","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.921354","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.921439","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.921525","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.921610","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.921695","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.929001","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.929150","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.929254","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.929335","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.929408","level":"info","event":"Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/yellow_tripdata_2024-01.parquet does not exist","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.929492","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.929578","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.929658","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.929756","level":"info","event":"\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.929841","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.929920","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.929999","level":"info","event":"\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.930077","level":"info","event":"\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.930155","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.930233","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.930313","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.930393","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.930488","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.930570","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.930648","level":"info","event":"\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.943643","level":"info","event":"\tat scala.util.Success.$anonfun$map$1(Try.scala:255)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.943909","level":"info","event":"\tat scala.util.Success.map(Try.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.944010","level":"info","event":"\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.944099","level":"info","event":"\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.944199","level":"info","event":"\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.944283","level":"info","event":"\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.944366","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.944447","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.944533","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.944613","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.944691","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.949984","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.950138","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.950230","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:21.950318","level":"info","event":"25/09/03 15:10:21 INFO DAGScheduler: Job 0 failed: parquet at NativeMethodAccessorImpl.java:0, took 4.702659 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.295963","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.296181","level":"info","event":"  File \"/opt/airflow/dags/hello_spark.py\", line 21, in <module>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.296281","level":"info","event":"    df = spark.read.parquet(\"/opt/airflow/data/yellow_tripdata_2024-01.parquet\")","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.296367","level":"info","event":"  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 544, in parquet","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.296449","level":"info","event":"  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.296534","level":"info","event":"  File \"/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.296612","level":"info","event":"  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.339414","level":"info","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o52.parquet.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.339899","level":"info","event":": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.22.0.5 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.340085","level":"info","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.340269","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.340387","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:383)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.340493","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.340577","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.340655","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.343815","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.344015","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.344135","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.344235","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.345016","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.345202","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.345319","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.345408","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.345491","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.345576","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.345655","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.345771","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.345859","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.345937","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.346012","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.346089","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.346164","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.346241","level":"info","event":"Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/yellow_tripdata_2024-01.parquet does not exist","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.346324","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.346403","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.346483","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.346564","level":"info","event":"\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.346639","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.348305","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.348655","level":"info","event":"\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.348987","level":"info","event":"\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.349098","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.349183","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.349265","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.349346","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.349424","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.349503","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.350303","level":"info","event":"\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.350600","level":"info","event":"\tat scala.util.Success.$anonfun$map$1(Try.scala:255)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.350905","level":"info","event":"\tat scala.util.Success.map(Try.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.351181","level":"info","event":"\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.351291","level":"info","event":"\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.351379","level":"info","event":"\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.351470","level":"info","event":"\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.351557","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.351645","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.352353","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.352475","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.352566","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.352648","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.352747","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.352835","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.352913","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.352994","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.353074","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.353153","level":"info","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.353236","level":"info","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.353330","level":"info","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.353414","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.353501","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.353589","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.353671","level":"info","event":"\tat scala.Option.foreach(Option.scala:407)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.355115","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.355451","level":"info","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.355782","level":"info","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.356091","level":"info","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.356435","level":"info","event":"\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.356731","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.357029","level":"info","event":"\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.357330","level":"info","event":"\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.357604","level":"info","event":"\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.357699","level":"info","event":"\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.357805","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.357886","level":"info","event":"\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.357973","level":"info","event":"\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.358075","level":"info","event":"\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.358178","level":"info","event":"\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.358277","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:73)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.358379","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.358482","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.358581","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.358682","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.360056","level":"info","event":"\tat scala.Option.orElse(Option.scala:447)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.360392","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.360508","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.360597","level":"info","event":"\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.360680","level":"info","event":"\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.360784","level":"info","event":"\tat scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.360865","level":"info","event":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.360955","level":"info","event":"\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.361039","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.361122","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.361205","level":"info","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.361287","level":"info","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.361371","level":"info","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.361454","level":"info","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.361538","level":"info","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.361617","level":"info","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.361694","level":"info","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.363256","level":"info","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.363590","level":"info","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.363902","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.364183","level":"info","event":"Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.364449","level":"info","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.364553","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.364639","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:383)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.364738","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.364837","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.364917","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.365000","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.365081","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.365155","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.365235","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.365311","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.365392","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.365472","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.365550","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.365627","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.365705","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.367202","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.367499","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.367802","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.368091","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.368388","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.368666","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.368976","level":"info","event":"\t... 1 more","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.369088","level":"info","event":"Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/yellow_tripdata_2024-01.parquet does not exist","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.369180","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.369265","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.369348","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.369427","level":"info","event":"\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.369504","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.369582","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.369662","level":"info","event":"\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.370678","level":"info","event":"\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.371017","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.371289","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.371564","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.371676","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.371787","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.371876","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.371962","level":"info","event":"\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.372045","level":"info","event":"\tat scala.util.Success.$anonfun$map$1(Try.scala:255)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.372137","level":"info","event":"\tat scala.util.Success.map(Try.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.372222","level":"info","event":"\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.372305","level":"info","event":"\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.372385","level":"info","event":"\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.372468","level":"info","event":"\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.372549","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.372633","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.373787","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.374154","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.374457","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.374757","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.375054","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.407296","level":"info","event":"25/09/03 15:10:22 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.407566","level":"info","event":"25/09/03 15:10:22 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.419599","level":"info","event":"25/09/03 15:10:22 INFO SparkUI: Stopped Spark web UI at http://6dcc95cec7e0:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.423670","level":"info","event":"25/09/03 15:10:22 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.424251","level":"info","event":"25/09/03 15:10:22 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.488176","level":"info","event":"25/09/03 15:10:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.515556","level":"info","event":"25/09/03 15:10:22 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.520970","level":"info","event":"25/09/03 15:10:22 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.570849","level":"info","event":"25/09/03 15:10:22 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.583666","level":"info","event":"25/09/03 15:10:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.617919","level":"info","event":"25/09/03 15:10:22 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.618096","level":"info","event":"25/09/03 15:10:22 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.618202","level":"info","event":"25/09/03 15:10:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-384e5105-33f1-4515-b6f2-115a3c0a5c4e/pyspark-0c8e9928-dd41-41b9-8dd9-dceb6fea487f","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.626954","level":"info","event":"25/09/03 15:10:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-7a033cc2-bf8b-4ef4-a7eb-3a481230e55e","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.633978","level":"info","event":"25/09/03 15:10:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-384e5105-33f1-4515-b6f2-115a3c0a5c4e","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.703039","level":"info","event":"Command exited with return code 1","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:10:22.703914","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Bash command failed. The command returned a non-zero exit code 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.10/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":838,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.10/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1130,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.10/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/standard/operators/bash.py","lineno":233,"name":"execute"}]}]}
