{"timestamp":"2025-09-03T15:40:02.784798","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-03T15:40:02.785780","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/spark_submit_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-03T15:40:02.805578","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:02.806309","level":"info","event":"Running command: ['/usr/bin/bash', '-c', '\\n        spark-submit         --master spark://spark-master:7077         --conf spark.hadoop.fs.s3a.access.key=telcoaz         --conf spark.hadoop.fs.s3a.secret.key=Telco12345         --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000         --conf spark.hadoop.fs.s3a.path.style.access=true \\t--conf spark.eventlog.enabled=false         /opt/airflow/dags/hello_spark.py\\n        ']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:02.826349","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.294565","level":"info","event":"25/09/03 15:40:07 INFO SparkContext: Running Spark version 3.5.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.300640","level":"info","event":"25/09/03 15:40:07 INFO SparkContext: OS info Linux, 6.14.0-1012-aws, amd64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.301829","level":"info","event":"25/09/03 15:40:07 INFO SparkContext: Java version 17.0.16","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.411058","level":"info","event":"25/09/03 15:40:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.544974","level":"info","event":"25/09/03 15:40:07 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.545218","level":"info","event":"25/09/03 15:40:07 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.547942","level":"info","event":"25/09/03 15:40:07 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.548158","level":"info","event":"25/09/03 15:40:07 INFO SparkContext: Submitted application: ProcessingTask","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.556606","level":"info","event":"25/09/03 15:40:07 INFO SparkContext: Spark configuration:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.556862","level":"info","event":"spark.app.name=ProcessingTask","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.556985","level":"info","event":"spark.app.startTime=1756914007276","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.557094","level":"info","event":"spark.app.submitTime=1756914005836","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.557197","level":"info","event":"spark.delta.logStore.class=org.apache.spark.sql.delta.storage.S3SingleDriverLogStore","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.557309","level":"info","event":"spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.557445","level":"info","event":"spark.eventLog.dir=/opt/spark/history","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.557551","level":"info","event":"spark.eventLog.enabled=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.557647","level":"info","event":"spark.eventlog.enabled=false","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.560881","level":"info","event":"spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.562296","level":"info","event":"spark.hadoop.fs.s3a.access.key=*********(redacted)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.562479","level":"info","event":"spark.hadoop.fs.s3a.endpoint=http://minio:9000","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.562593","level":"info","event":"spark.hadoop.fs.s3a.fast.upload=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.562698","level":"info","event":"spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.562829","level":"info","event":"spark.hadoop.fs.s3a.multiobjectdelete.enable=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.562934","level":"info","event":"spark.hadoop.fs.s3a.path.style.access=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.563037","level":"info","event":"spark.hadoop.fs.s3a.secret.key=*********(redacted)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.563136","level":"info","event":"spark.history.fs.logDirectory=s3a://spark/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.563234","level":"info","event":"spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.563331","level":"info","event":"spark.jars.repositories=https://packages.confluent.io/maven/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.563427","level":"info","event":"spark.logConf=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.563524","level":"info","event":"spark.master=spark://spark-master:7077","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.563619","level":"info","event":"spark.network.timeout=10000s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.566253","level":"info","event":"spark.rdd.compress=True","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.566542","level":"info","event":"spark.serializer.objectStreamReset=100","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.566676","level":"info","event":"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.566798","level":"info","event":"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.566894","level":"info","event":"spark.sql.files.ignoreMissingFiles=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.567868","level":"info","event":"spark.sql.sources.partitionOverwriteMode=dynamic","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.568015","level":"info","event":"spark.submit.deployMode=client","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.568145","level":"info","event":"spark.submit.pyFiles=","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.611794","level":"info","event":"25/09/03 15:40:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.626535","level":"info","event":"25/09/03 15:40:07 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.627171","level":"info","event":"25/09/03 15:40:07 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.754970","level":"info","event":"25/09/03 15:40:07 INFO SecurityManager: Changing view acls to: airflow","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.755787","level":"info","event":"25/09/03 15:40:07 INFO SecurityManager: Changing modify acls to: airflow","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.757059","level":"info","event":"25/09/03 15:40:07 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.760227","level":"info","event":"25/09/03 15:40:07 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:07.762575","level":"info","event":"25/09/03 15:40:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:08.223638","level":"info","event":"25/09/03 15:40:08 INFO Utils: Successfully started service 'sparkDriver' on port 32829.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:08.277995","level":"info","event":"25/09/03 15:40:08 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:08.327085","level":"info","event":"25/09/03 15:40:08 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:08.347661","level":"info","event":"25/09/03 15:40:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:08.348245","level":"info","event":"25/09/03 15:40:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:08.353345","level":"info","event":"25/09/03 15:40:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:08.379048","level":"info","event":"25/09/03 15:40:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-22d9fcba-394d-44af-9a4f-08bb66576728","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:08.398981","level":"info","event":"25/09/03 15:40:08 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:08.425062","level":"info","event":"25/09/03 15:40:08 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:08.667384","level":"info","event":"25/09/03 15:40:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:08.762970","level":"info","event":"25/09/03 15:40:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:09.175681","level":"info","event":"25/09/03 15:40:09 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:09.274642","level":"info","event":"25/09/03 15:40:09 INFO TransportClientFactory: Successfully created connection to spark-master/172.22.0.3:7077 after 55 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:09.465538","level":"info","event":"25/09/03 15:40:09 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250903154009-0016","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:09.480931","level":"info","event":"25/09/03 15:40:09 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250903154009-0016/0 on worker-20250903081951-172.22.0.5-43269 (172.22.0.5:43269) with 2 core(s)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:09.485359","level":"info","event":"25/09/03 15:40:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42269.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:09.485561","level":"info","event":"25/09/03 15:40:09 INFO NettyBlockTransferService: Server created on 31a98e31d62d:42269","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:09.488958","level":"info","event":"25/09/03 15:40:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:09.490771","level":"info","event":"25/09/03 15:40:09 INFO StandaloneSchedulerBackend: Granted executor ID app-20250903154009-0016/0 on hostPort 172.22.0.5:43269 with 2 core(s), 1024.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:09.506961","level":"info","event":"25/09/03 15:40:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 31a98e31d62d, 42269, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:09.512979","level":"info","event":"25/09/03 15:40:09 INFO BlockManagerMasterEndpoint: Registering block manager 31a98e31d62d:42269 with 434.4 MiB RAM, BlockManagerId(driver, 31a98e31d62d, 42269, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:09.513916","level":"info","event":"25/09/03 15:40:09 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250903154009-0016/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:09.516769","level":"info","event":"25/09/03 15:40:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 31a98e31d62d, 42269, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:09.519969","level":"info","event":"25/09/03 15:40:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 31a98e31d62d, 42269, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:10.093964","level":"info","event":"25/09/03 15:40:10 INFO SingleEventLogFileWriter: Logging events to file:/opt/spark/history/app-20250903154009-0016.inprogress","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:10.628948","level":"info","event":"25/09/03 15:40:10 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:11.234138","level":"info","event":"25/09/03 15:40:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:11.241153","level":"info","event":"25/09/03 15:40:11 INFO SharedState: Warehouse path is 'file:/tmp/airflowtmpxhutki7n/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:13.774567","level":"info","event":"25/09/03 15:40:13 INFO InMemoryFileIndex: It took 133 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:15.217210","level":"info","event":"25/09/03 15:40:15 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:15.320975","level":"info","event":"25/09/03 15:40:15 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:15.321209","level":"info","event":"25/09/03 15:40:15 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:15.321975","level":"info","event":"25/09/03 15:40:15 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:15.327466","level":"info","event":"25/09/03 15:40:15 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:15.342965","level":"info","event":"25/09/03 15:40:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:15.705197","level":"info","event":"25/09/03 15:40:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 105.5 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:15.841997","level":"info","event":"25/09/03 15:40:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.3 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:15.846801","level":"info","event":"25/09/03 15:40:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 31a98e31d62d:42269 (size: 38.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:15.859754","level":"info","event":"25/09/03 15:40:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:15.924991","level":"info","event":"25/09/03 15:40:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:15.932970","level":"info","event":"25/09/03 15:40:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:18.166532","level":"info","event":"25/09/03 15:40:18 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.22.0.5:39252) with ID 0,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:18.688350","level":"info","event":"25/09/03 15:40:18 INFO BlockManagerMasterEndpoint: Registering block manager 172.22.0.5:42045 with 434.4 MiB RAM, BlockManagerId(0, 172.22.0.5, 42045, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:18.909546","level":"info","event":"25/09/03 15:40:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 7768 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:19.942433","level":"info","event":"25/09/03 15:40:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.22.0.5:42045 (size: 38.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.856017","level":"info","event":"25/09/03 15:40:21 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.22.0.5 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.856241","level":"info","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.856374","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.856480","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:383)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.856577","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.856676","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.862997","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.863177","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.863285","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.863381","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.863481","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.863568","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.863659","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.874915","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.875157","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.875273","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.875372","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.875465","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.875561","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.875661","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.875790","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.875883","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.875972","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.876058","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.876151","level":"info","event":"Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/yellow_tripdata_2024-01.parquet does not exist","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.876236","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.876323","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.876405","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.876481","level":"info","event":"\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.876561","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.876641","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.890065","level":"info","event":"\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.890368","level":"info","event":"\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.890496","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.890590","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.890683","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.890801","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.890892","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.891023","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.891136","level":"info","event":"\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.891229","level":"info","event":"\tat scala.util.Success.$anonfun$map$1(Try.scala:255)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.891319","level":"info","event":"\tat scala.util.Success.map(Try.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.891410","level":"info","event":"\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.891497","level":"info","event":"\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.891583","level":"info","event":"\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.891670","level":"info","event":"\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.932966","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.933185","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.933295","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.933394","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.933487","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.933578","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.933682","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:21.936021","level":"info","event":"25/09/03 15:40:21 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 7768 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.082532","level":"info","event":"25/09/03 15:40:22 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on 172.22.0.5, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.085150","level":"info","event":"25/09/03 15:40:22 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 7768 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.232948","level":"info","event":"25/09/03 15:40:22 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on 172.22.0.5, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.246727","level":"info","event":"25/09/03 15:40:22 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 7768 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.374947","level":"info","event":"25/09/03 15:40:22 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on 172.22.0.5, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.382880","level":"info","event":"25/09/03 15:40:22 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.383120","level":"info","event":"25/09/03 15:40:22 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.406368","level":"info","event":"25/09/03 15:40:22 INFO TaskSchedulerImpl: Cancelling stage 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.407504","level":"info","event":"25/09/03 15:40:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.22.0.5 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.407754","level":"info","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.407885","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.407983","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:383)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.408085","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.408186","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.408266","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.408346","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.408421","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.408500","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.408575","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.408645","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.414745","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.415000","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.415105","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.415192","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.415274","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.415357","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.415436","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.415518","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.415595","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.415670","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.421406","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.421634","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.424848","level":"info","event":"Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/yellow_tripdata_2024-01.parquet does not exist","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.425081","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.425186","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.425273","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.425358","level":"info","event":"\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.425439","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.425519","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.425600","level":"info","event":"\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.425678","level":"info","event":"\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.429994","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.430184","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.430284","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.430376","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.430460","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.430541","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.430626","level":"info","event":"\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.433827","level":"info","event":"\tat scala.util.Success.$anonfun$map$1(Try.scala:255)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.434134","level":"info","event":"\tat scala.util.Success.map(Try.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.434248","level":"info","event":"\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.434337","level":"info","event":"\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.434421","level":"info","event":"\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.434516","level":"info","event":"\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.434595","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.434672","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.434778","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.434860","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.434937","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.435014","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.435092","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.435173","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.435249","level":"info","event":"25/09/03 15:40:22 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) failed in 6.983 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.22.0.5 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.435329","level":"info","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.435404","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.435477","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:383)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.435550","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.435625","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.435700","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.443184","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.443331","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.443423","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.443508","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.443587","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.443668","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.443778","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.443861","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.443940","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.444020","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.446600","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.449841","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.450095","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.450202","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.450290","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.450385","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.450469","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.450551","level":"info","event":"Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/yellow_tripdata_2024-01.parquet does not exist","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.450633","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.450735","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.450830","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.450909","level":"info","event":"\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.450980","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.451048","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.451116","level":"info","event":"\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.451186","level":"info","event":"\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.451265","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.451348","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.451429","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.451510","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.451609","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.459217","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.459469","level":"info","event":"\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.459571","level":"info","event":"\tat scala.util.Success.$anonfun$map$1(Try.scala:255)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.459656","level":"info","event":"\tat scala.util.Success.map(Try.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.459756","level":"info","event":"\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.459839","level":"info","event":"\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.459917","level":"info","event":"\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.459993","level":"info","event":"\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.460065","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.460158","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.460237","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.460312","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.460384","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.460456","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.460528","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.460603","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.460687","level":"info","event":"25/09/03 15:40:22 INFO DAGScheduler: Job 0 failed: parquet at NativeMethodAccessorImpl.java:0, took 7.235412 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.803234","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.805888","level":"info","event":"  File \"/opt/airflow/dags/hello_spark.py\", line 21, in <module>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.806403","level":"info","event":"    df = spark.read.parquet(\"/opt/airflow/data/yellow_tripdata_2024-01.parquet\")","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.806740","level":"info","event":"  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 544, in parquet","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.807068","level":"info","event":"  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.807375","level":"info","event":"  File \"/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.807683","level":"info","event":"  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.846461","level":"info","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o52.parquet.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.846670","level":"info","event":": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.22.0.5 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.846794","level":"info","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.846886","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.846974","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:383)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.847060","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.847141","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.847226","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.847310","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.847390","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.847470","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.847550","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.847630","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.847731","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.847821","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.847916","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.847999","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.848079","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.848192","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.848281","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.848370","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.848460","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.848550","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.848641","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.855289","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.855517","level":"info","event":"Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/yellow_tripdata_2024-01.parquet does not exist","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.855637","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.855761","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.855866","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.855963","level":"info","event":"\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.856054","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.856152","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.856237","level":"info","event":"\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.856322","level":"info","event":"\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.856408","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.856495","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.856579","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.856678","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.858073","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.858206","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.858303","level":"info","event":"\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.858395","level":"info","event":"\tat scala.util.Success.$anonfun$map$1(Try.scala:255)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.858485","level":"info","event":"\tat scala.util.Success.map(Try.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.858576","level":"info","event":"\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.858669","level":"info","event":"\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.858787","level":"info","event":"\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.858900","level":"info","event":"\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.858998","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.859095","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.859191","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.859286","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.859381","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.859475","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.859570","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.859664","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.863925","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.864123","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.864236","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.864330","level":"info","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.864419","level":"info","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.864507","level":"info","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.864596","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.864682","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.864795","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.864889","level":"info","event":"\tat scala.Option.foreach(Option.scala:407)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.864978","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.865067","level":"info","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.865153","level":"info","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.865241","level":"info","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.865332","level":"info","event":"\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.865422","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.865505","level":"info","event":"\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.865590","level":"info","event":"\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.865678","level":"info","event":"\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.869942","level":"info","event":"\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.870101","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.870191","level":"info","event":"\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.870283","level":"info","event":"\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.870371","level":"info","event":"\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.870456","level":"info","event":"\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.870545","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:73)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.870641","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.870752","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.870839","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.870917","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.870993","level":"info","event":"\tat scala.Option.orElse(Option.scala:447)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.871070","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.871150","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.871230","level":"info","event":"\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.871307","level":"info","event":"\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.871392","level":"info","event":"\tat scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.871483","level":"info","event":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.871566","level":"info","event":"\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.871650","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.875865","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.876100","level":"info","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.876235","level":"info","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.876345","level":"info","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.876433","level":"info","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.876517","level":"info","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.876602","level":"info","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.876688","level":"info","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.877731","level":"info","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.877862","level":"info","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.877960","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.878050","level":"info","event":"Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.878138","level":"info","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.878222","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.878310","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:383)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.878401","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.878495","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.878592","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.878688","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.881772","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.881914","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.882017","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.882111","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.882207","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.882301","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.882394","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.882484","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.882575","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.882666","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.882777","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.882861","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.882938","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.887002","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.887126","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.887230","level":"info","event":"\t... 1 more","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.887331","level":"info","event":"Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/yellow_tripdata_2024-01.parquet does not exist","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.887430","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.887522","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.887613","level":"info","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.887723","level":"info","event":"\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.887811","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.887887","level":"info","event":"\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.887965","level":"info","event":"\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.888044","level":"info","event":"\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.888134","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.888218","level":"info","event":"\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.888296","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.888377","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.888457","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.888540","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.888622","level":"info","event":"\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.888703","level":"info","event":"\tat scala.util.Success.$anonfun$map$1(Try.scala:255)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.893018","level":"info","event":"\tat scala.util.Success.map(Try.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.893170","level":"info","event":"\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.893270","level":"info","event":"\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.893360","level":"info","event":"\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.893439","level":"info","event":"\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.893517","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.893601","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.893684","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.893786","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.893874","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.893957","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.894037","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.975935","level":"info","event":"25/09/03 15:40:22 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:22.977946","level":"info","event":"25/09/03 15:40:22 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.018926","level":"info","event":"25/09/03 15:40:23 INFO SparkUI: Stopped Spark web UI at http://31a98e31d62d:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.024792","level":"info","event":"25/09/03 15:40:23 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.028924","level":"info","event":"25/09/03 15:40:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.109069","level":"info","event":"25/09/03 15:40:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.158400","level":"info","event":"25/09/03 15:40:23 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.159974","level":"info","event":"25/09/03 15:40:23 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.184618","level":"info","event":"25/09/03 15:40:23 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.212274","level":"info","event":"25/09/03 15:40:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.252459","level":"info","event":"25/09/03 15:40:23 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.254940","level":"info","event":"25/09/03 15:40:23 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.257826","level":"info","event":"25/09/03 15:40:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-f81852d6-a97d-41b5-8564-982000423c6e","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.277690","level":"info","event":"25/09/03 15:40:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-1db3fbd4-d943-4e32-b82f-bdb84ad6eb3d","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.294092","level":"info","event":"25/09/03 15:40:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-1db3fbd4-d943-4e32-b82f-bdb84ad6eb3d/pyspark-dd90b5aa-300b-4a2c-a822-adf722a7fd74","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.407747","level":"info","event":"Command exited with return code 1","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-09-03T15:40:23.409059","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Bash command failed. The command returned a non-zero exit code 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.10/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":838,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.10/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1130,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.10/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/standard/operators/bash.py","lineno":233,"name":"execute"}]}]}
